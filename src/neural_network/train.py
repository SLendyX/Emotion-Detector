import os
import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, EarlyStopping

# --- CONFIGURARE ---
DATA_DIR = "data/processed"
MODELS_DIR = "models"
BATCH_SIZE = 64
EPOCHS = 50  # NumÄƒrul de treceri prin tot setul de date
NUM_CLASSES = 7  # Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise

def build_model(input_shape):
    """
    Definim arhitectura CNN.
    Este o arhitecturÄƒ stil VGG (blocuri de convoluÈ›ie urmate de pooling).
    """
    model = Sequential()

    # --- Bloc 1: DetecÈ›ie trÄƒsÄƒturi de bazÄƒ ---
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
    model.add(BatchNormalization()) # NormalizeazÄƒ datele intern pentru vitezÄƒ
    model.add(MaxPooling2D(pool_size=(2, 2))) # MicÈ™oreazÄƒ imaginea la jumÄƒtate
    model.add(Dropout(0.25)) # "UitÄƒ" aleatoriu 25% din neuroni (evitÄƒ tocitul/overfitting)

    # --- Bloc 2: DetecÈ›ie trÄƒsÄƒturi medii (ochi, gurÄƒ) ---
    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    # --- Bloc 3: Clasificare (Creierul decizional) ---
    model.add(Flatten()) # TransformÄƒ matricea 3D Ã®n vector 1D
    model.add(Dense(1024, activation='relu'))
    model.add(Dropout(0.5)) # Dropout agresiv Ã®nainte de final
    model.add(Dense(NUM_CLASSES, activation='softmax')) # Softmax ne dÄƒ probabilitÄƒÈ›ile (suma lor = 1)

    # Compilarea modelului
    # Folosim Adam (cel mai bun optimizer general) È™i Categorical Crossentropy (pentru clasificare multiplÄƒ)
    model.compile(loss='categorical_crossentropy',
                  optimizer=Adam(learning_rate=0.0001),
                  metrics=['accuracy'])
    
    return model

def main():
    # 1. ÃncÄƒrcÄƒm datele procesate
    print("ğŸ”„ ÃncÄƒrcare date din .npy...")
    try:
        X_train = np.load(os.path.join(DATA_DIR, "X_train.npy"))
        y_train = np.load(os.path.join(DATA_DIR, "y_train.npy"))
        X_test = np.load(os.path.join(DATA_DIR, "X_test.npy"))
        y_test = np.load(os.path.join(DATA_DIR, "y_test.npy"))
    except FileNotFoundError:
        print("âŒ Nu am gÄƒsit fiÈ™ierele .npy. RuleazÄƒ preprocessing-ul Ã®ntÃ¢i!")
        return

    # VerificÄƒm forma datelor (trebuie sÄƒ fie ex: (28000, 48, 48, 1))
    print(f"Dimensiune Train: {X_train.shape}")
    input_shape = X_train.shape[1:] # (48, 48, 1)

    # 2. Construim modelul
    model = build_model(input_shape)
    model.summary() # AfiÈ™eazÄƒ structura Ã®n consolÄƒ

    # 3. PregÄƒtim callback-urile (SalvÄƒri automate)
    if not os.path.exists(MODELS_DIR):
        os.makedirs(MODELS_DIR)
    
    # SalvÄƒm DOAR cel mai bun model (care are cea mai micÄƒ eroare pe setul de test)
    checkpoint = ModelCheckpoint(
        os.path.join(MODELS_DIR, 'emotion_model.keras'), # extensia nouÄƒ keras
        monitor='val_loss',
        save_best_only=True,
        mode='min',
        verbose=1
    )

    # Oprim antrenarea dacÄƒ nu mai Ã®nvaÈ›Äƒ nimic timp de 7 epoci (economie de timp)
    early_stop = EarlyStopping(
        monitor='val_loss',
        patience=7,
        restore_best_weights=True,
        verbose=1
    )

    # 4. START ANTRENARE ğŸš€
    print("\nğŸš€ Ãncepem antrenarea! Ia-È›i o cafea, dureazÄƒ...")
    history = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=(X_test, y_test),
        callbacks=[checkpoint, early_stop],
        shuffle=True
    )

    print("\nâœ… Antrenare finalizatÄƒ.")
    
    # Putem salva È™i istoricul pentru a plota graficele mai tÃ¢rziu
    np.save(os.path.join(MODELS_DIR, 'history.npy'), history.history)

if __name__ == "__main__":
    main()